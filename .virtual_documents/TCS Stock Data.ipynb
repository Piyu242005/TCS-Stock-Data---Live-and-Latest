





# Install required packages if not already installed
import subprocess
import sys

def install_package(package):
    subprocess.check_call([sys.executable, "-m", "pip", "install", package, "-q"])

# Install torch if not available
try:
    import torch
except ImportError:
    print("Installing PyTorch...")
    install_package("torch")
    import torch

# Import Required Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Machine Learning Libraries
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import MinMaxScaler

# Deep Learning Libraries (PyTorch)
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset

# For saving models
import pickle

# Set display options
pd.set_option('display.max_columns', None)
pd.set_option('display.width', None)
plt.style.use('seaborn-v0_8-whitegrid')

# Check if GPU is available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

print("âœ… All libraries imported successfully!")
print(f"ðŸ“… Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print(f"ðŸ–¥ï¸ Device: {device}")
print(f"ðŸ Python: {sys.executable}")





# Load the TCS stock history data
df = pd.read_csv('TCS_stock_history.csv')

# Convert Date column to datetime format
df['Date'] = pd.to_datetime(df['Date'])

# Sort by date in chronological order
df = df.sort_values('Date').reset_index(drop=True)

# Display basic information
print("ðŸ“Š TCS Stock Data Loaded Successfully!")
print(f"ðŸ“ˆ Total Records: {len(df)}")
print(f"ðŸ“… Date Range: {df['Date'].min().strftime('%Y-%m-%d')} to {df['Date'].max().strftime('%Y-%m-%d')}")
print(f"ðŸ“‹ Columns: {list(df.columns)}")
print("\nðŸ” First 5 rows:")
df.head()


# Display data types and info
print("ðŸ“‹ Data Types and Info:")
print(df.info())
print("\nðŸ“Š Statistical Summary:")
df.describe()





# Check for missing values
print("ðŸ” Missing Values Check:")
missing_values = df.isnull().sum()
print(missing_values)
print(f"\nðŸ“Š Total Missing Values: {missing_values.sum()}")

# Check for duplicates
duplicates = df.duplicated().sum()
print(f"\nðŸ”„ Duplicate Rows: {duplicates}")


# Handle missing values using forward fill
df_clean = df.copy()
df_clean = df_clean.fillna(method='ffill')

# Handle any remaining missing values with backward fill
df_clean = df_clean.fillna(method='bfill')

# Remove rows where Volume is 0 (non-trading days) - optional
# df_clean = df_clean[df_clean['Volume'] > 0]

# Check for outliers using IQR method
def detect_outliers(data, column):
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]
    return len(outliers)

print("ðŸ“Š Outlier Detection (IQR Method):")
for col in ['Open', 'High', 'Low', 'Close', 'Volume']:
    outlier_count = detect_outliers(df_clean, col)
    print(f"   {col}: {outlier_count} outliers")

print("\nâœ… Data preprocessing completed!")
print(f"ðŸ“ˆ Clean dataset shape: {df_clean.shape}")





# 4.1 Stock Price Trend Analysis
fig, axes = plt.subplots(2, 2, figsize=(16, 10))

# Close Price Trend
axes[0, 0].plot(df_clean['Date'], df_clean['Close'], color='blue', linewidth=0.8)
axes[0, 0].set_title('TCS Close Price Over Time', fontsize=14, fontweight='bold')
axes[0, 0].set_xlabel('Date')
axes[0, 0].set_ylabel('Close Price (â‚¹)')
axes[0, 0].grid(True, alpha=0.3)

# Open Price Trend
axes[0, 1].plot(df_clean['Date'], df_clean['Open'], color='green', linewidth=0.8)
axes[0, 1].set_title('TCS Open Price Over Time', fontsize=14, fontweight='bold')
axes[0, 1].set_xlabel('Date')
axes[0, 1].set_ylabel('Open Price (â‚¹)')
axes[0, 1].grid(True, alpha=0.3)

# High Price Trend
axes[1, 0].plot(df_clean['Date'], df_clean['High'], color='red', linewidth=0.8)
axes[1, 0].set_title('TCS High Price Over Time', fontsize=14, fontweight='bold')
axes[1, 0].set_xlabel('Date')
axes[1, 0].set_ylabel('High Price (â‚¹)')
axes[1, 0].grid(True, alpha=0.3)

# Low Price Trend
axes[1, 1].plot(df_clean['Date'], df_clean['Low'], color='orange', linewidth=0.8)
axes[1, 1].set_title('TCS Low Price Over Time', fontsize=14, fontweight='bold')
axes[1, 1].set_xlabel('Date')
axes[1, 1].set_ylabel('Low Price (â‚¹)')
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.suptitle('TCS Stock Price Trends (2002-2024)', fontsize=16, fontweight='bold', y=1.02)
plt.show()


# 4.2 Volume Analysis
fig, ax = plt.subplots(figsize=(14, 5))
ax.bar(df_clean['Date'], df_clean['Volume'], color='purple', alpha=0.7, width=2)
ax.set_title('TCS Trading Volume Over Time', fontsize=14, fontweight='bold')
ax.set_xlabel('Date')
ax.set_ylabel('Volume')
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Volume Statistics
print("ðŸ“Š Volume Statistics:")
print(f"   Mean Volume: {df_clean['Volume'].mean():,.0f}")
print(f"   Max Volume: {df_clean['Volume'].max():,.0f}")
print(f"   Min Volume: {df_clean['Volume'].min():,.0f}")


# 4.3 Dividends and Stock Splits Analysis
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Dividends
dividend_data = df_clean[df_clean['Dividends'] > 0]
axes[0].scatter(dividend_data['Date'], dividend_data['Dividends'], color='green', alpha=0.7, s=50)
axes[0].set_title('TCS Dividends Over Time', fontsize=14, fontweight='bold')
axes[0].set_xlabel('Date')
axes[0].set_ylabel('Dividend Amount')
axes[0].grid(True, alpha=0.3)

# Stock Splits
split_data = df_clean[df_clean['Stock Splits'] > 0]
axes[1].scatter(split_data['Date'], split_data['Stock Splits'], color='red', alpha=0.7, s=100)
axes[1].set_title('TCS Stock Splits Over Time', fontsize=14, fontweight='bold')
axes[1].set_xlabel('Date')
axes[1].set_ylabel('Split Ratio')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(f"ðŸ“Š Total Dividend Events: {len(dividend_data)}")
print(f"ðŸ“Š Total Stock Split Events: {len(split_data)}")


# 4.4 Correlation Heatmap
fig, ax = plt.subplots(figsize=(10, 8))
correlation_matrix = df_clean[['Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits']].corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, 
            fmt='.3f', linewidths=0.5, ax=ax)
ax.set_title('Correlation Heatmap - TCS Stock Features', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

print("\nðŸ“Š Key Correlation Insights:")
print("   - Open, High, Low, and Close prices are highly correlated (>0.99)")
print("   - Volume shows weak correlation with prices")
print("   - Dividends and Stock Splits have minimal correlation with daily prices")


# 4.5 Moving Averages Analysis
df_clean['MA_30'] = df_clean['Close'].rolling(window=30).mean()
df_clean['MA_50'] = df_clean['Close'].rolling(window=50).mean()
df_clean['MA_200'] = df_clean['Close'].rolling(window=200).mean()

fig, ax = plt.subplots(figsize=(16, 8))
ax.plot(df_clean['Date'], df_clean['Close'], label='Close Price', alpha=0.7, linewidth=0.8)
ax.plot(df_clean['Date'], df_clean['MA_30'], label='30-Day MA', color='orange', linewidth=1.5)
ax.plot(df_clean['Date'], df_clean['MA_50'], label='50-Day MA', color='green', linewidth=1.5)
ax.plot(df_clean['Date'], df_clean['MA_200'], label='200-Day MA', color='red', linewidth=1.5)

ax.set_title('TCS Stock Price with Moving Averages', fontsize=14, fontweight='bold')
ax.set_xlabel('Date')
ax.set_ylabel('Price (â‚¹)')
ax.legend(loc='upper left')
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

print("ðŸ“Š Moving Averages help identify:")
print("   - 30-Day MA: Short-term trend")
print("   - 50-Day MA: Medium-term trend")
print("   - 200-Day MA: Long-term trend")


# 4.6 Daily Percentage Change Analysis
df_clean['Daily_Return'] = df_clean['Close'].pct_change() * 100

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Daily Returns Distribution
axes[0].hist(df_clean['Daily_Return'].dropna(), bins=100, color='blue', alpha=0.7, edgecolor='black')
axes[0].axvline(x=0, color='red', linestyle='--', linewidth=2)
axes[0].set_title('Distribution of Daily Returns', fontsize=14, fontweight='bold')
axes[0].set_xlabel('Daily Return (%)')
axes[0].set_ylabel('Frequency')
axes[0].grid(True, alpha=0.3)

# Daily Returns Over Time
axes[1].plot(df_clean['Date'], df_clean['Daily_Return'], color='green', alpha=0.5, linewidth=0.5)
axes[1].axhline(y=0, color='red', linestyle='--', linewidth=1)
axes[1].set_title('Daily Returns Over Time', fontsize=14, fontweight='bold')
axes[1].set_xlabel('Date')
axes[1].set_ylabel('Daily Return (%)')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("ðŸ“Š Daily Return Statistics:")
print(f"   Mean Daily Return: {df_clean['Daily_Return'].mean():.4f}%")
print(f"   Std Deviation: {df_clean['Daily_Return'].std():.4f}%")
print(f"   Max Daily Gain: {df_clean['Daily_Return'].max():.2f}%")
print(f"   Max Daily Loss: {df_clean['Daily_Return'].min():.2f}%")





# 5.1 Extract Date Features
df_clean['Year'] = df_clean['Date'].dt.year
df_clean['Month'] = df_clean['Date'].dt.month
df_clean['Day'] = df_clean['Date'].dt.day
df_clean['Weekday'] = df_clean['Date'].dt.weekday  # Monday=0, Sunday=6
df_clean['Quarter'] = df_clean['Date'].dt.quarter

# 5.2 Create Lag Features
df_clean['Prev_Close'] = df_clean['Close'].shift(1)
df_clean['Prev_Open'] = df_clean['Open'].shift(1)
df_clean['Prev_Volume'] = df_clean['Volume'].shift(1)

# 5.3 Create Price Range Features
df_clean['High_Low_Range'] = df_clean['High'] - df_clean['Low']
df_clean['Open_Close_Range'] = df_clean['Close'] - df_clean['Open']

# 5.4 Volatility Features
df_clean['Volatility_30'] = df_clean['Daily_Return'].rolling(window=30).std()

# 5.5 Trading Signal based on MA Crossover (Simple Strategy)
df_clean['Signal'] = 0
df_clean.loc[df_clean['MA_30'] > df_clean['MA_200'], 'Signal'] = 1  # Buy Signal
df_clean.loc[df_clean['MA_30'] < df_clean['MA_200'], 'Signal'] = -1  # Sell Signal

print("âœ… Feature Engineering Completed!")
print(f"\nðŸ“Š New Features Created:")
print(f"   Date Features: Year, Month, Day, Weekday, Quarter")
print(f"   Lag Features: Prev_Close, Prev_Open, Prev_Volume")
print(f"   Price Features: High_Low_Range, Open_Close_Range")
print(f"   Volatility Features: Volatility_30")
print(f"   Trading Signal: Based on MA Crossover")

print(f"\nðŸ“‹ Updated Dataset Shape: {df_clean.shape}")
df_clean.head()





# Prepare data for Linear Regression
# Select features for prediction
features = ['Open', 'High', 'Low', 'Volume', 'Prev_Close', 'Month', 'Weekday']
target = 'Close'

# Remove rows with NaN values (due to lag features)
df_ml = df_clean.dropna(subset=features + [target]).copy()

print(f"ðŸ“Š Dataset for ML Model:")
print(f"   Total samples: {len(df_ml)}")
print(f"   Features: {features}")
print(f"   Target: {target}")

# Split features and target
X = df_ml[features]
y = df_ml[target]

# Split into training and testing sets (80-20 split)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)

print(f"\nðŸ“ˆ Train-Test Split:")
print(f"   Training samples: {len(X_train)}")
print(f"   Testing samples: {len(X_test)}")


# Train Linear Regression Model
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)

# Make predictions
y_pred_train = lr_model.predict(X_train)
y_pred_test = lr_model.predict(X_test)

# Evaluate the model
train_mse = mean_squared_error(y_train, y_pred_train)
test_mse = mean_squared_error(y_test, y_pred_test)
train_mae = mean_absolute_error(y_train, y_pred_train)
test_mae = mean_absolute_error(y_test, y_pred_test)
train_r2 = r2_score(y_train, y_pred_train)
test_r2 = r2_score(y_test, y_pred_test)

print("ðŸ“Š Linear Regression Model Performance:")
print("\n   Training Metrics:")
print(f"      MSE: {train_mse:.4f}")
print(f"      MAE: {train_mae:.4f}")
print(f"      RÂ² Score: {train_r2:.4f}")

print("\n   Testing Metrics:")
print(f"      MSE: {test_mse:.4f}")
print(f"      MAE: {test_mae:.4f}")
print(f"      RÂ² Score: {test_r2:.4f}")

print(f"\n   RMSE (Test): {np.sqrt(test_mse):.4f}")


# Visualize Linear Regression Results
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Actual vs Predicted (Test Set)
axes[0].scatter(y_test, y_pred_test, alpha=0.5, color='blue', s=10)
axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2)
axes[0].set_xlabel('Actual Close Price')
axes[0].set_ylabel('Predicted Close Price')
axes[0].set_title('Actual vs Predicted (Linear Regression)', fontsize=14, fontweight='bold')
axes[0].grid(True, alpha=0.3)

# Prediction over Time (Test Set)
test_dates = df_ml.iloc[-len(y_test):]['Date'].values
axes[1].plot(test_dates, y_test.values, label='Actual', color='blue', linewidth=1)
axes[1].plot(test_dates, y_pred_test, label='Predicted', color='red', linewidth=1, alpha=0.7)
axes[1].set_xlabel('Date')
axes[1].set_ylabel('Close Price')
axes[1].set_title('Actual vs Predicted Over Time (Test Set)', fontsize=14, fontweight='bold')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Feature Importance
print("\nðŸ“Š Feature Coefficients (Linear Regression):")
for feature, coef in zip(features, lr_model.coef_):
    print(f"   {feature}: {coef:.6f}")





# Prepare data for LSTM
# Use Close price for prediction
close_data = df_clean['Close'].dropna().values.reshape(-1, 1)

# Normalize the data
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(close_data)

# Create sequences for LSTM
def create_sequences(data, seq_length):
    X, y = [], []
    for i in range(seq_length, len(data)):
        X.append(data[i-seq_length:i, 0])
        y.append(data[i, 0])
    return np.array(X), np.array(y)

# Define sequence length (look-back period)
sequence_length = 60

X_lstm, y_lstm = create_sequences(scaled_data, sequence_length)

# Reshape for LSTM [samples, time steps, features]
X_lstm = X_lstm.reshape(X_lstm.shape[0], X_lstm.shape[1], 1)

# Split into train and test sets (80-20)
split_index = int(len(X_lstm) * 0.8)
X_train_lstm = X_lstm[:split_index]
X_test_lstm = X_lstm[split_index:]
y_train_lstm = y_lstm[:split_index]
y_test_lstm = y_lstm[split_index:]

# Convert to PyTorch tensors
X_train_tensor = torch.FloatTensor(X_train_lstm).to(device)
y_train_tensor = torch.FloatTensor(y_train_lstm).reshape(-1, 1).to(device)
X_test_tensor = torch.FloatTensor(X_test_lstm).to(device)
y_test_tensor = torch.FloatTensor(y_test_lstm).reshape(-1, 1).to(device)

# Create DataLoader for batch training
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

print("ðŸ“Š LSTM Data Preparation:")
print(f"   Sequence Length: {sequence_length} days")
print(f"   Total Sequences: {len(X_lstm)}")
print(f"   Training Sequences: {len(X_train_lstm)}")
print(f"   Testing Sequences: {len(X_test_lstm)}")
print(f"   Input Shape: {X_lstm.shape}")


# Build LSTM Model using PyTorch
class LSTMModel(nn.Module):
    def __init__(self, input_size=1, hidden_size=50, num_layers=3, dropout=0.2):
        super(LSTMModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, 
                           batch_first=True, dropout=dropout)
        self.fc1 = nn.Linear(hidden_size, 25)
        self.fc2 = nn.Linear(25, 1)
        
    def forward(self, x):
        # LSTM output
        lstm_out, _ = self.lstm(x)
        # Take the output from the last time step
        out = lstm_out[:, -1, :]
        out = self.fc1(out)
        out = self.fc2(out)
        return out

# Initialize model
lstm_model = LSTMModel(input_size=1, hidden_size=50, num_layers=3, dropout=0.2).to(device)

# Loss function and optimizer
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.001)

# Display model summary
print("ðŸ“Š LSTM Model Architecture (PyTorch):")
print(lstm_model)
print(f"\nðŸ“Š Total Parameters: {sum(p.numel() for p in lstm_model.parameters()):,}")


# Train the LSTM Model
print("ðŸš€ Training LSTM Model...")
print("   This may take a few minutes...\n")

num_epochs = 25
train_losses = []
val_losses = []

for epoch in range(num_epochs):
    lstm_model.train()
    epoch_loss = 0
    
    for batch_X, batch_y in train_loader:
        # Forward pass
        outputs = lstm_model(batch_X)
        loss = criterion(outputs, batch_y)
        
        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        epoch_loss += loss.item()
    
    # Calculate average training loss
    avg_train_loss = epoch_loss / len(train_loader)
    train_losses.append(avg_train_loss)
    
    # Validation loss
    lstm_model.eval()
    with torch.no_grad():
        val_outputs = lstm_model(X_test_tensor)
        val_loss = criterion(val_outputs, y_test_tensor).item()
        val_losses.append(val_loss)
    
    if (epoch + 1) % 5 == 0:
        print(f"   Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.6f}, Val Loss: {val_loss:.6f}")

print("\nâœ… LSTM Model Training Completed!")


# Plot Training History
fig, ax = plt.subplots(figsize=(12, 5))
ax.plot(train_losses, label='Training Loss', color='blue')
ax.plot(val_losses, label='Validation Loss', color='red')
ax.set_title('LSTM Model Training History', fontsize=14, fontweight='bold')
ax.set_xlabel('Epoch')
ax.set_ylabel('Loss (MSE)')
ax.legend()
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()


# Make Predictions with LSTM
lstm_model.eval()
with torch.no_grad():
    predictions_lstm = lstm_model(X_test_tensor).cpu().numpy()

# Inverse transform predictions
predictions_lstm = scaler.inverse_transform(predictions_lstm)

# Get actual values
y_test_actual = scaler.inverse_transform(y_test_lstm.reshape(-1, 1))

# Calculate metrics for LSTM
lstm_mse = mean_squared_error(y_test_actual, predictions_lstm)
lstm_mae = mean_absolute_error(y_test_actual, predictions_lstm)
lstm_rmse = np.sqrt(lstm_mse)

print("ðŸ“Š LSTM Model Performance (Test Set):")
print(f"   MSE: {lstm_mse:.4f}")
print(f"   MAE: {lstm_mae:.4f}")
print(f"   RMSE: {lstm_rmse:.4f}")


# Visualize LSTM Predictions
fig, ax = plt.subplots(figsize=(16, 8))

# Get dates for test set
test_dates_lstm = df_clean['Date'].values[sequence_length + split_index:]

ax.plot(test_dates_lstm, y_test_actual, label='Actual Price', color='blue', linewidth=1.5)
ax.plot(test_dates_lstm, predictions_lstm, label='LSTM Predicted', color='red', linewidth=1.5, alpha=0.8)

ax.set_title('TCS Stock Price: Actual vs LSTM Predicted', fontsize=14, fontweight='bold')
ax.set_xlabel('Date')
ax.set_ylabel('Close Price (â‚¹)')
ax.legend(loc='upper left')
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()





# Model Comparison Summary
print("=" * 60)
print("ðŸ“Š MODEL COMPARISON SUMMARY")
print("=" * 60)

print("\nðŸ”¹ LINEAR REGRESSION MODEL:")
print(f"   â€¢ Mean Squared Error (MSE): {test_mse:.4f}")
print(f"   â€¢ Mean Absolute Error (MAE): {test_mae:.4f}")
print(f"   â€¢ Root Mean Squared Error (RMSE): {np.sqrt(test_mse):.4f}")
print(f"   â€¢ RÂ² Score: {test_r2:.4f}")

print("\nðŸ”¹ LSTM DEEP LEARNING MODEL:")
print(f"   â€¢ Mean Squared Error (MSE): {lstm_mse:.4f}")
print(f"   â€¢ Mean Absolute Error (MAE): {lstm_mae:.4f}")
print(f"   â€¢ Root Mean Squared Error (RMSE): {lstm_rmse:.4f}")

print("\n" + "=" * 60)
print("ðŸ“ˆ KEY INSIGHTS:")
print("=" * 60)
print("   â€¢ Linear Regression provides quick baseline predictions")
print("   â€¢ LSTM captures temporal patterns in stock data")
print("   â€¢ Both models show strong correlation with actual prices")
print("   â€¢ Stock prediction remains challenging due to market volatility")
print("=" * 60)





# Save Linear Regression Model using pickle
with open('tcs_linear_regression_model.pkl', 'wb') as file:
    pickle.dump(lr_model, file)
print("âœ… Linear Regression model saved as 'tcs_linear_regression_model.pkl'")

# Save LSTM Model (PyTorch)
torch.save(lstm_model.state_dict(), 'tcs_lstm_model.pth')
print("âœ… LSTM model saved as 'tcs_lstm_model.pth'")

# Save the scaler for future use
with open('tcs_scaler.pkl', 'wb') as file:
    pickle.dump(scaler, file)
print("âœ… Scaler saved as 'tcs_scaler.pkl'")

# Export prediction results to CSV
results_df = pd.DataFrame({
    'Date': test_dates_lstm,
    'Actual_Close': y_test_actual.flatten(),
    'LSTM_Predicted': predictions_lstm.flatten()
})
results_df.to_csv('tcs_prediction_results.csv', index=False)
print("âœ… Prediction results exported to 'tcs_prediction_results.csv'")





# Predict next 30 days using LSTM
def predict_future(model, data, scaler, n_days, seq_length, device):
    """Predict future stock prices using PyTorch LSTM"""
    model.eval()
    predictions = []
    current_seq = torch.FloatTensor(data[-seq_length:].reshape(1, seq_length, 1)).to(device)
    
    with torch.no_grad():
        for _ in range(n_days):
            pred = model(current_seq)
            predictions.append(pred.item())
            # Update sequence
            new_seq = current_seq[:, 1:, :]
            pred_reshaped = pred.reshape(1, 1, 1)
            current_seq = torch.cat([new_seq, pred_reshaped], dim=1)
    
    # Inverse transform
    predictions = scaler.inverse_transform(np.array(predictions).reshape(-1, 1))
    return predictions.flatten()

# Predict next 30 days
n_future_days = 30
future_predictions = predict_future(lstm_model, scaled_data, scaler, n_future_days, sequence_length, device)

# Create future dates
last_date = df_clean['Date'].iloc[-1]
future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=n_future_days, freq='B')

# Create DataFrame for future predictions
future_df = pd.DataFrame({
    'Date': future_dates,
    'Predicted_Close': future_predictions
})

print("ðŸ“ˆ Next 30 Days Price Prediction:")
print(future_df.to_string(index=False))

# Plot future predictions
fig, ax = plt.subplots(figsize=(14, 6))

# Plot last 90 days of actual data
recent_data = df_clean.tail(90)
ax.plot(recent_data['Date'], recent_data['Close'], label='Historical Close', color='blue', linewidth=1.5)

# Plot predictions
ax.plot(future_dates, future_predictions, label='Future Prediction (30 days)', color='red', 
        linewidth=2, linestyle='--', marker='o', markersize=3)

ax.set_title('TCS Stock Price: Historical + 30-Day Forecast', fontsize=14, fontweight='bold')
ax.set_xlabel('Date')
ax.set_ylabel('Close Price (â‚¹)')
ax.legend()
ax.grid(True, alpha=0.3)
ax.axvline(x=last_date, color='green', linestyle='--', linewidth=1, label='Prediction Start')
plt.tight_layout()
plt.show()

print(f"\nðŸ“Š Last Known Close Price: â‚¹{df_clean['Close'].iloc[-1]:.2f}")
print(f"ðŸ“ˆ Predicted Price in 30 Days: â‚¹{future_predictions[-1]:.2f}")



